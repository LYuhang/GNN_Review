{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN的Batch示例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题简介**：由于GNN处理的数据通常来说是不规则、格式不统一的图(graph)，因此，如何将数据进行批处理并输入到神经网络中进行训练是一个比较常见的问题，该代码使用`对角邻接矩阵`的方式来实现批处理问题(受到了PyG框架的启发)。该代码的数据集使用人工生成的图分类数据集，并使用Pytorch框架进行实现数据载入、模型构建、训练、评估等流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任务定义和数据集生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "子图匹配分类：给定一个子图(subgraph)$g$以及图(graph)的数据集$\\mathcal{G}=\\{G_1,G_2,...,G_n\\}$，对应的标签为$\\mathcal{Y}=\\{y_1,y_2,...,y_n\\}$，对于任意的图(graph)$G_i$及其标签$y_i$，有：\n",
    "$$\n",
    "\\begin{equation}\n",
    "y_i=\\left\\{\n",
    "\\begin{aligned}\n",
    "1 & \\text{ }G_i包含子图g \\\\\n",
    "0 & \\text{ }G_i不包含子图g \\\\\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图(graph)都可以定义为$m$个节点的集合$\\mathcal{N}=\\{v_1,v_2,...,v_m\\}$和$n$条边的集合$\\mathcal{E}=\\{e_1,e_2,...,e_n\\}$，其中，边的数据结构为两个节点的元组，即$(v_i,v_j)$。现设定数据集： \n",
    "+ 有26种节点：A,B,C,...,Z。每一种节点都有特定的特征向量，比如one-hot。\n",
    "+ 图(graph)由不定数量的上述类型节点和不定数量连接的边构成。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**：\n",
    "+ 为了生成代码所需的字典和数据文件，需要运行下面的单元。\n",
    "  + 生成训练集：修改第53行的`graph_num=10000`，变量以及第64行的`random.seed(0)`。表示使用随机种子0来生成包含有10000个graph的训练集。\n",
    "  + 生成验证集：修改第53行的`graph_num=2000`，变量以及第64行的`random.seed(1)`。表示使用随机种子1来生成包含有2000个graph的验证集。\n",
    "+ 生成的字典和数据集文件保存在`/PyG和Pytorch实现GNN模型/data/`文件夹下，该文件夹下有三个文件：\n",
    "```\n",
    "/PyG和Pytorch实现GNN模型/data/nodes_dict.json\n",
    "/PyG和Pytorch实现GNN模型/data/dataset.json\n",
    "/PyG和Pytorch实现GNN模型/data/dataset_val.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving node dict...\n",
      "Successfully saving dict!\n",
      "1000 graphs have been generated!\n",
      "2000 graphs have been generated!\n",
      "Saving dataset!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Code for dataset generation.\n",
    "\"\"\"\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "\n",
    "\"\"\"\n",
    "Generating nodes dict.\n",
    "\"\"\"\n",
    "if not os.path.exists(\"./data/\"):\n",
    "    os.mkdir(\"./data/\")\n",
    "node_types = list(string.ascii_uppercase)\n",
    "nodes_dict = dict([(k, v) for v, k in enumerate(node_types)])\n",
    "nodes_dict_path = \"./data/nodes_dict.json\"\n",
    "\n",
    "print(\"Saving node dict...\")\n",
    "with open(nodes_dict_path, \"w\") as fp:\n",
    "    json.dump({\n",
    "        \"itos\" : node_types,\n",
    "        \"stoi\" : nodes_dict\n",
    "    }, fp)\n",
    "print(\"Successfully saving dict!\")\n",
    "\n",
    "\"\"\"\n",
    "Show graph.\n",
    "Input : \n",
    "    g : tuple(list, list)\n",
    "\"\"\"\n",
    "def show_graph(g):\n",
    "    labels = dict([(k,v) for k, v in enumerate(g[0])])\n",
    "    nodes = range(len(g[0]))\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(nodes)\n",
    "    G.add_edges_from(g[1])\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw(G, pos)\n",
    "    nx.draw_networkx_labels(G, pos, labels)\n",
    "\n",
    "subgraph = ([\"A\", \"A\", \"B\", \"C\"], \n",
    "            [(0, 1),\n",
    "             (0, 2),\n",
    "             (1, 2),\n",
    "             (2, 3)])\n",
    "min_nodes_num = 5\n",
    "max_nodes_num = 50\n",
    "graph_num = 10000\n",
    "\n",
    "\"\"\"\n",
    "Generating graph dataset. There are three steps:\n",
    "Step1 : Randomly choose number of nodes(N).\n",
    "Step2 : Generate random graph with edge number ranging from N-1 to N * (N - 1) / 2.\n",
    "Step3 : Remove unconnected graph.\n",
    "Step4 : Add subgraph to some graphs.\n",
    "\"\"\"\n",
    "N = 0\n",
    "graphs = []\n",
    "random.seed(0)\n",
    "while N < graph_num:\n",
    "    node_num = random.randint(min_nodes_num, max_nodes_num)\n",
    "    edge_num = random.randint(node_num-1, node_num * (node_num - 1) / 2)\n",
    "    G = nx.random_graphs.dense_gnm_random_graph(node_num, edge_num)\n",
    "    if nx.connected.is_connected(G):\n",
    "        graphs.append(G)\n",
    "        N += 1\n",
    "        if N % 1000 == 0:\n",
    "            print(\"{} graphs have been generated!\".format(N))\n",
    "\n",
    "\"\"\"\n",
    "Transform nx.Graph into our graph type.\n",
    "\"\"\"\n",
    "def transform_nx_graph(g):\n",
    "    nodes = random.choices(population=node_types, k=len(g.nodes))\n",
    "    edges = list(g.edges)\n",
    "    \n",
    "    return (nodes, edges)\n",
    "\n",
    "\"\"\"\n",
    "Merge subgraph into graph.\n",
    "\"\"\"\n",
    "def merge_subgraph_into_graph(g, sg):\n",
    "    g_node_num = len(g[0])\n",
    "    sg_node_num = len(sg[0])\n",
    "    \n",
    "    merge_edges = [(s+g_node_num, d+g_node_num) for s, d in sg[1]]\n",
    "    \n",
    "    g_range = range(g_node_num)\n",
    "    sg_range = range(g_node_num, g_node_num+sg_node_num)\n",
    "    new_edges_num = random.randint(1, g_node_num)\n",
    "    src_list = random.choices(population=g_range, k=new_edges_num)\n",
    "    dst_list = random.choices(population=sg_range, k=new_edges_num)\n",
    "    new_edges = [(s, d) for s, d in zip(src_list, dst_list)]\n",
    "    new_edges = list(set(new_edges))\n",
    "    \n",
    "    merge_graph = (g[0]+sg[0],\n",
    "                   g[1]+merge_edges+new_edges)\n",
    "    \n",
    "    return merge_graph\n",
    "\n",
    "graphs = [transform_nx_graph(g) for g in graphs]\n",
    "graphs_with_sg = [(merge_subgraph_into_graph(g, subgraph), 1) for g in graphs[:len(graphs)//2]]\n",
    "graphs_without_sg = [(g, 0) for g in graphs[len(graphs)//2:]]\n",
    "graphs = graphs_with_sg + graphs_without_sg\n",
    "random.shuffle(graphs)\n",
    "\n",
    "print(\"Saving dataset!\")\n",
    "dataset_path = \"./data/dataset_val.json\"\n",
    "with open(dataset_path, \"w\") as fp:\n",
    "    json.dump({\n",
    "        \"time\" : time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()),\n",
    "        \"subgraph\" : subgraph,\n",
    "        \"graphs\" : graphs,\n",
    "        \"min_nodes_num\" : min_nodes_num,\n",
    "        \"max_nodes_num\" : max_nodes_num,\n",
    "        \"graphs_num\" : graph_num\n",
    "    }, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader定义 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "dict_path = \"./data/nodes_dict.json\"\n",
    "graph_path = \"./data/dataset.json\"\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, dict_path, graph_path):\n",
    "        super(GraphDataset, self).__init__()\n",
    "        assert os.path.exists(dict_path) and os.path.exists(graph_path)\n",
    "        \n",
    "        with open(dict_path, \"r\") as fp:\n",
    "            self.node_dict = json.load(fp)[\"stoi\"]\n",
    "        with open(graph_path, \"r\") as fp:\n",
    "            self.graphs = json.load(fp)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.graphs[\"graphs\"])\n",
    "    \n",
    "    def __getitem__(self, ind):\n",
    "        graph, label = self.graphs[\"graphs\"][ind]\n",
    "        nodes = [self.node_dict[n] for n in graph[0]]\n",
    "        edges = [list(e) for e in graph[1]]\n",
    "        \n",
    "        return np.array(nodes, dtype=np.int64), np.array(edges, dtype=np.int64), np.array([label], dtype=np.int64)\n",
    "\n",
    "\"\"\"\n",
    "Transform edges into adjacency matrix.\n",
    "Input :\n",
    "    node_num : Total num of nodes.\n",
    "    edges : Edges matrix.\n",
    "Output :\n",
    "    m : Adjacency matrix.\n",
    "\"\"\"\n",
    "def edges_to_matrix(node_num, edges):\n",
    "    m = np.zeros(shape=(node_num, node_num), dtype=np.uint8)\n",
    "    m[edges[:,0], edges[:,1]] = 1\n",
    "    m[edges[:,1], edges[:,0]] = 1\n",
    "    m[np.arange(node_num), np.arange(node_num)] = 1\n",
    "    \n",
    "    return m\n",
    "\n",
    "\"\"\"\n",
    "Combine multiple graphs into one large graph.\n",
    "\"\"\"\n",
    "def collate_fn(batch):\n",
    "    nodes_list = [b[0] for b in batch]\n",
    "    nodes = np.concatenate(nodes_list, axis=0)\n",
    "    \n",
    "    nodes_lens = np.fromiter(map(lambda l: l.shape[0], nodes_list), dtype=np.int64)\n",
    "    nodes_inds = np.cumsum(nodes_lens)\n",
    "    nodes_num = nodes_inds[-1]\n",
    "    nodes_inds = np.insert(nodes_inds, 0, 0)\n",
    "    nodes_inds = np.delete(nodes_inds, -1)\n",
    "    edges_list = [b[1] for b in batch]\n",
    "    edges_list = [e+i for e,i in zip(edges_list, nodes_inds)]\n",
    "    edges = np.concatenate(edges_list, axis=0)\n",
    "    m = edges_to_matrix(nodes_num, edges)\n",
    "    \n",
    "    labels = [b[2] for b in batch]\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    \n",
    "    batch_mask = [np.array([i]*k, dtype=np.int32) for i, k in zip(range(len(batch)), nodes_lens)]\n",
    "    batch_mask = np.concatenate(batch_mask, axis=0)\n",
    "    return torch.from_numpy(nodes), torch.from_numpy(m).float(), torch.from_numpy(labels), torch.from_numpy(batch_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据PyG文档中对GCN的公式表示：\n",
    "$$\n",
    "x_i^{(k)}=\\sum_{j\\in{}\\mathcal{N(i)}\\cup{}\\{i\\}}\\frac{1}{\\sqrt{\\textrm{deg}(i)}\\cdot{}\\sqrt{\\textrm{deg}(j)}}\\cdot{}(\\Theta\\cdot{}x_j^{(k-1)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\"\"\"\n",
    "Implement of GCN module.\n",
    "\"\"\"\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, dropout=0.):\n",
    "        super(GCN, self).__init__()\n",
    "        self.trans_msg = nn.Linear(in_dim, out_dim)\n",
    "        self.nonlinear = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    \"\"\"\n",
    "    Input : \n",
    "        x : (N, in_dim)\n",
    "        m : (N, N)\n",
    "    Output :\n",
    "        out : (N, out_dim)\n",
    "    \"\"\"\n",
    "    def forward(self, x:torch.Tensor, m:torch.Tensor):\n",
    "        x_msg = self.trans_msg(x)\n",
    "        x_msg = self.nonlinear(x_msg)\n",
    "        x_msg = self.dropout(x_msg)\n",
    "        \n",
    "        row_degree = torch.sum(m, dim=1, keepdim=True)   # (N, 1)\n",
    "        col_degree = torch.sum(m, dim=0, keepdim=True)   # (1, N)\n",
    "        degree = torch.mm(torch.sqrt(row_degree), torch.sqrt(col_degree))  # (N, N)\n",
    "        out = torch.mm(m / degree, x_msg)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\"\"\"\n",
    "Implement of GCN network.\n",
    "\"\"\"\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, nodes_num, embedding_dim, hidden_dims, num_classes, dropout=0.):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.node_embedding = nn.Embedding(nodes_num, embedding_dim)\n",
    "        gcns = []\n",
    "        in_dim = embedding_dim\n",
    "        for d in hidden_dims:\n",
    "            gcns.append(GCN(in_dim, d, dropout))\n",
    "            in_dim = d\n",
    "        self.gcns = nn.ModuleList(gcns)\n",
    "        \n",
    "        self.classifier = nn.Linear(in_dim, num_classes)\n",
    "    \n",
    "    \"\"\"\n",
    "    Input :\n",
    "        x : (N, out_dim)\n",
    "        bm : (N, )\n",
    "    Output :\n",
    "        out : (batch_size, out_dim)\n",
    "    \"\"\"\n",
    "    def gcn_maxpooling(self, x, bm):\n",
    "        batch_size = torch.max(bm)+1\n",
    "        out = []\n",
    "        for i in range(batch_size):\n",
    "            inds = (bm == i).nonzero()[:,0]\n",
    "            x_ind = torch.index_select(x, dim=0, index=inds)\n",
    "            out.append(torch.max(x_ind, dim=0, keepdim=False)[0])\n",
    "        out = torch.stack(out, dim=0)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def gcn_meanpooling(self, x, lens):\n",
    "        batch_size = torch.max(bm)+1\n",
    "        out = []\n",
    "        for i in range(batch_size):\n",
    "            inds = (bm == i).nonzero()[:,0]\n",
    "            x_ind = torch.index_select(x, dim=0, index=inds)\n",
    "            out.append(torch.mean(x_ind, dim=0, keepdim=False))\n",
    "        out = torch.stack(out, dim=0)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def gcn_sumpooling(self, x, lens):\n",
    "        batch_size = torch.max(bm)+1\n",
    "        out = []\n",
    "        for i in range(batch_size):\n",
    "            inds = (bm == i).nonzero()[:,0]\n",
    "            x_ind = torch.index_select(x, dim=0, index=inds)\n",
    "            out.append(torch.sum(x_ind, dim=0, keepdim=False))\n",
    "        out = torch.stack(out, dim=0)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \"\"\"\n",
    "    Input :\n",
    "        x : (N, )\n",
    "        m : (N, N)\n",
    "        bm : (N, )\n",
    "    Output :\n",
    "        output : (batch_size, num_classes)\n",
    "    \"\"\"\n",
    "    def forward(self, x, m, bm):\n",
    "        x_emb = self.node_embedding(x)  # (N, embedding_dim)\n",
    "        out = x_emb\n",
    "        for ml in self.gcns:\n",
    "            out = ml(out, m)\n",
    "        output = self.gcn_maxpooling(out, bm)   # (batch_size, out_dim)\n",
    "        \n",
    "        logits = self.classifier(output)  # (batch_size, num_classes)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**：先运行`Dataloader定义`和`GCN模型构建`单元。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Net(nodes_num=len(ds.node_dict), embedding_dim=20, hidden_dims=[32], num_classes=2, dropout=0.2)\n",
    "model = model.cuda() if torch.cuda.is_available() else model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.)\n",
    "batch_size = 5\n",
    "\n",
    "ds = GraphDataset(dict_path, graph_path)\n",
    "dataloader = DataLoader(dataset=ds,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False,\n",
    "                        num_workers=0,\n",
    "                        collate_fn=collate_fn,\n",
    "                        drop_last=False)\n",
    "\n",
    "ds_val = GraphDataset(dict_path, \"./data/dataset_val.json\")\n",
    "dataloader_val = DataLoader(dataset=ds_val,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False,\n",
    "                            num_workers=0,\n",
    "                            collate_fn=collate_fn,\n",
    "                            drop_last=False)\n",
    "\n",
    "def eval_model(dataloader, model):\n",
    "    right = 0\n",
    "    total = 0\n",
    "    for n, m, l, bm in dataloader:\n",
    "        n = n.cuda()\n",
    "        m = m.cuda()\n",
    "        l = l.cuda()\n",
    "        bm = bm.cuda()\n",
    "        \n",
    "        logits = model(n, m, bm)\n",
    "        \n",
    "        preds = torch.max(logits, dim=-1, keepdim=False)[1] == l\n",
    "        right += torch.sum(preds).cpu().detach().data\n",
    "        total += preds.shape[0]\n",
    "    \n",
    "    return float(right) / total\n",
    "\n",
    "epochs = 20\n",
    "steps = len(ds) // batch_size\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for ep in range(epochs):\n",
    "    step = 0\n",
    "    for n, m, l, bm in dataloader:\n",
    "        n = n.cuda()\n",
    "        m = m.cuda()\n",
    "        l = l.cuda()\n",
    "        bm = bm.cuda()\n",
    "        \n",
    "        logits = model(n, m, bm)\n",
    "        \n",
    "        loss = loss_fn(logits, l)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(\"Epoch {}, step {}/{} : loss {}\".format(ep, step, steps, loss.detach().cpu().data))\n",
    "        step += 1\n",
    "    model.eval()\n",
    "    acc = eval_model(dataloader_val, model)\n",
    "    print(\"Epoch {}, val accuracy : {:.4f}\".format(ep, acc))\n",
    "    model.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
